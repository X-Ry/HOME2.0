---
title: 'Senior Experience Machine Learning Section 4 - Simple Linear Regression'
date: '2019-11-25 5:31pm'
description: 'Senior Experience Machine Learning Section 4'
author: x-ry	

layout: post
comments: false
hidden: false
published: true 

surfaceTags: false
showDate: true
---

**Simple Linear Regression Intuition**

So simple linear regression creates a formula:

y =  b<sub>0 </sub>+ b<sub>1</sub>*x<sub>1</sub>

y = dependent var, x = independent var (input),  

b<sub>1 </sub>= **coefficient** that will determine how a change in the unit X1 will affect the dependent variable

b<sub>0 </sub>= **constant** 

In short it’s finding the line of best fit.

**ok so how does simple linear regression find the line of best fit?**

<img src="https://x-ry.github.io/assets/images/posts/ml/4graph.png" alt="best fit line" width="200"/>

It minimizes the sum of the distances.

Specifically, it wants to minimize (SUM (y<sub>point</sub> - y<sub>line</sub>)<sup>2</sup>), and whatever b’s it finds that minimize the most are the b’s you want!

And that’s called the **Ordinary Least Squares method**

OK i remember learning this before, but why is it the sum of the values squared and not just sum?

The <span style="text-decoration:underline;">sum of squares determines deviation</span> from the line. That’s why it’s a sum of the differences squared and not just sums of differences.

